## Шпаргалка по линейной регрессии

### 1. Основы линейной регрессии
Линейная регрессия — это статистический метод, используемый для моделирования зависимости между одной зависимой переменной \(Y\) и одной или несколькими независимыми переменными \(X\). Основная идея заключается в нахождении линейной зависимости между переменными.

### 2. Математическая модель
Формула линейной регрессии для одной переменной выглядит следующим образом:

\[
Y = \beta_0 + \beta_1 X + \epsilon
\]

где:
- \(Y\) — зависимая переменная (целевое значение),
- \(X\) — независимая переменная,
- \(\beta_0\) — свободный член (пересечение с осью Y),
- \(\beta_1\) — коэффициент наклона (изменение \(Y\) при изменении \(X\)),
- \(\epsilon\) — ошибка модели (разность между предсказанным и фактическим значением).

### 3. Множественная линейная регрессия
Для нескольких независимых переменных модель записывается как:

\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\]

где \(X_1, X_2, \ldots, X_n\) — независимые переменные.

### 4. Оценка параметров модели
Коэффициенты модели \(\beta\) оцениваются с помощью метода наименьших квадратов, минимизирующего сумму квадратов ошибок:

\[
\hat{\beta} = \arg \min \sum_{i=1}^{m} (Y_i - \hat{Y_i})^2
\]

где \(\hat{Y_i} = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_n X_{ni}\).

### 5. Оценка качества модели
- **Коэффициент детерминации \(R^2\)**: Показывает долю вариации зависимой переменной, объясненную независимыми переменными:
  
\[
R^2 = 1 - \frac{\sum_{i=1}^{m} (Y_i - \hat{Y_i})^2}{\sum_{i=1}^{m} (Y_i - \bar{Y})^2}
\]

где \(\bar{Y}\) — среднее значение зависимой переменной.

- **Средняя квадратичная ошибка (MSE)**:

\[
MSE = \frac{1}{m} \sum_{i=1}^{m} (Y_i - \hat{Y_i})^2
\]

- **Средняя абсолютная ошибка (MAE)**:

\[
MAE = \frac{1}{m} \sum_{i=1}^{m} |Y_i - \hat{Y_i}|
\]

### 6. Допущения линейной регрессии
1. **Линейность**: Существует линейная зависимость между независимыми и зависимой переменными.
2. **Нормальность**: Ошибки (остатки) нормально распределены.
3. **Гомоскедастичность**: Постоянная дисперсия ошибок.
4. **Отсутствие мультиколлинеарности**: Независимые переменные не должны быть сильно коррелированы между собой.

### 7. Визуализация
- **График зависимости**: График с точками, представляющими наблюдения, и линией регрессии, показывающей предсказанное значение.

### 8. Пример использования
На практике линейная регрессия может быть использована для:
- Прогнозирования цен на жилье на основе характеристик (площадь, количество комнат и т. д.).
- Оценки влияния маркетинговых затрат на продажи.

### Заключение
Линейная регрессия является мощным инструментом для анализа взаимосвязей между переменными. Она проста в реализации и интерпретации, что делает ее одним из наиболее популярных методов в статистике и машинном обучении.
